{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "from functions.data_etl.scrapy.datacenters_com.spiders.datacenters_com import DataCenterSpider\n",
    "from functions.data_etl.web_scraping import fetch_datacenter_com_master_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set csv paths\n",
    "DATA_PATH_ROOT = Path(\"data/outputs/0_webscraping/\")\n",
    "BASIC_INFO_CSV_PATH = DATA_PATH_ROOT / \"datacenters_com_basic_info.csv\"\n",
    "DETAILED_INFO_CSV_PATH = DATA_PATH_ROOT / \"datacenters_com_details.csv\"\n",
    "FULL_INFO_CSV_PATH = DATA_PATH_ROOT / \"datacenters_com.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch basic datacenter info\n",
    "basic_info = fetch_datacenter_com_master_list(BASIC_INFO_CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Scrapy spider output path\n",
    "scrapy_settings = get_project_settings()\n",
    "scrapy_settings.update({\"FEEDS\": {str(DETAILED_INFO_CSV_PATH.resolve()): {\"format\": \"csv\", \"overwrite\": True}}})\n",
    "\n",
    "# Create the crawler process and run the Scrapy spider to fetch detailed info\n",
    "scrapy_process = CrawlerProcess(scrapy_settings)\n",
    "scrapy_process.crawl(DataCenterSpider)\n",
    "scrapy_process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge back the data from the two sources and output the final dataset\n",
    "basic_info = pd.read_csv(BASIC_INFO_CSV_PATH)\n",
    "detailed_info = pd.read_csv(DETAILED_INFO_CSV_PATH)\n",
    "\n",
    "full_info = basic_info.merge(detailed_info, on=\"url\")\n",
    "full_info = full_info.drop(columns=[\"url\"])\n",
    "\n",
    "full_info.to_csv(FULL_INFO_CSV_PATH, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
