{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning, merging, and geocoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from haversine import Unit, haversine\n",
    "\n",
    "from functions.data_etl.cleaning import convert_area_to_sqm\n",
    "from functions.data_etl.geocoding import extract_geocoded_data_from_txt, geocode_addresses\n",
    "from functions.data_etl.imputation import PowerCapacityScenario, impute_missing_values\n",
    "from functions.data_etl.merging import generate_fuzzy_matches\n",
    "from functions.project_settings import WGS84_CRS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting input and output paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY_BOUNDARIES_INPUT = Path(\"data/inputs/common/country_boundaries/ne_110m_admin_0_countries.shp\")\n",
    "DATACENTERS_COM_INPUT = Path(\"data/outputs/0_webscraping/datacenters_com.csv\")\n",
    "DATACENTERMAP_INPUT = Path(\"data/outputs/0_webscraping/datacentermap.com.xlsx\")\n",
    "\n",
    "OUTPUT_DIR = Path(\"data/outputs/1_data_etl/\")\n",
    "GEOCODING_API_RESULTS_PATH = OUTPUT_DIR / \"geocoding_results.txt\"\n",
    "DATACENTERMAP_GEOCODED_OUTPUT = OUTPUT_DIR / \"datacentermap_geocoded.csv\"\n",
    "\n",
    "## Manual edits\n",
    "# For adding missing country codes\n",
    "DATACENTERMAP_GDF_PRE_MANUAL_EDIT_1 = OUTPUT_DIR / \"datacentermap_v1_pre_edit_gdf.csv\"\n",
    "DATACENTERS_COM_GDF_PRE_MANUAL_EDIT_1 = OUTPUT_DIR / \"datacenters_com_v1_pre_edit_gdf.csv\"\n",
    "DATACENTERMAP_GDF_POST_MANUAL_EDIT_1 = OUTPUT_DIR / \"datacentermap_v1_post_edit_gdf.csv\"\n",
    "DATACENTERS_COM_GDF_POST_MANUAL_EDIT_1 = OUTPUT_DIR / \"datacenters_com_v1_post_edit_gdf.csv\"\n",
    "\n",
    "# For fixing duplicate datacentermap entries\n",
    "DATACENTERMAP_GDF_DUPLICATES_PRE_MANUAL_EDIT = OUTPUT_DIR / \"datacentermap_v2_pre_edit_gdf.csv\"\n",
    "DATACENTERMAP_GDF_DUPLICATES_POST_MANUAL_EDIT = OUTPUT_DIR / \"datacentermap_v2_post_edit_gdf.csv\"\n",
    "\n",
    "# For fuzzy matching manual inspection\n",
    "FUZZY_MATCHES_PRE_MANUAL_EDIT = OUTPUT_DIR / \"fuzzy_matches_pre_manual_edit.csv\"\n",
    "FUZZY_MATCHES_POST_MANUAL_EDIT = OUTPUT_DIR / \"fuzzy_matches_post_manual_edit.csv\"\n",
    "\n",
    "# For manual data collection for Amazon, Google, Meta, Microsoft, and Apple\n",
    "DATA_CENTERS_MISSING_SPECS_PRE_MANUAL_EDIT = OUTPUT_DIR / \"data_centers_missing_specs_pre_edit.csv\"\n",
    "DATA_CENTERS_MISSING_SPECS_POST_MANUAL_EDIT = OUTPUT_DIR / \"data_centers_missing_specs_post_edit.csv\"\n",
    "DATA_CENTERS_WITH_SPECS_PRE_MANUAL_EDIT = OUTPUT_DIR / \"data_centers_with_specs_pre_edit.csv\"\n",
    "DATA_CENTERS_WITH_SPECS_POST_MANUAL_EDIT = OUTPUT_DIR / \"data_centers_with_specs_post_edit.csv\"\n",
    "\n",
    "## Final output prefix (will have min, max, avg suffixes)\n",
    "DATACENTERS_FINAL_OUTPUT_PREFIX = OUTPUT_DIR / \"data_centers_\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geocoding\n",
    "\n",
    "The web scraped datacenters.com information has latitude and longitude provided, so we only geocode datacentermap.com.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datacentermap.com data\n",
    "datacentermap = pd.read_excel(\n",
    "    DATACENTERMAP_INPUT,\n",
    "    usecols=[\"company\", \"name\", \"address\", \"total_space\", \"white_space\", \"critical_power_mw\", \"notes\"],\n",
    ")\n",
    "\n",
    "# Extract the addresses\n",
    "addresses_datacentermap = datacentermap[\"address\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geocode the addresses and store the results in a text file\n",
    "google_maps_api_key = input(\"Please enter your Google Maps API key: \")\n",
    "geocode_addresses(addresses_datacentermap, google_maps_api_key, GEOCODING_API_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing\n",
    "\n",
    "Geocoding is expensive and time intensive, so we save results to a text file in the case we have to do multiple rounds of geocoding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the address, latitude, and longitude from the text file\n",
    "geocoded_info = extract_geocoded_data_from_txt(GEOCODING_API_RESULTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the address column from the datacentermap data\n",
    "datacentermap = datacentermap.drop(columns=[\"address\"])\n",
    "\n",
    "# Append the geocoded information to the datacentermap dataframe\n",
    "datacentermap_geocoded = pd.concat(\n",
    "    [datacentermap, geocoded_info], axis=1\n",
    ")  # Ideally we would merge on the ID instead of concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results to a CSV file\n",
    "datacentermap_geocoded.to_csv(DATACENTERMAP_GEOCODED_OUTPUT, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datacenters.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "datacenters_com = pd.read_csv(DATACENTERS_COM_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "datacenters_com = datacenters_com.drop_duplicates(subset=[\"company\", \"name\", \"address\"])\n",
    "\n",
    "# Convert the area columns from square feet to square meters\n",
    "for col_name in [\"total_space_sqft\", \"colocation_space_sqft\"]:\n",
    "    datacenters_com[col_name.replace(\"sqft\", \"m2\")] = (\n",
    "        datacenters_com[col_name].str.replace(\",\", \"\").astype(float) * 0.092903  # 1 sqft = 0.092903 m2\n",
    "    )\n",
    "    datacenters_com = datacenters_com.drop(columns=[col_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datacentermap.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the geocoded datacentermap data\n",
    "datacentermap = pd.read_csv(DATACENTERMAP_GEOCODED_OUTPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the total and white space to square meters\n",
    "datacentermap[\"total_space_m2\"] = datacentermap[\"total_space\"].apply(convert_area_to_sqm)\n",
    "datacentermap[\"white_space_m2\"] = datacentermap[\"white_space\"].apply(convert_area_to_sqm)\n",
    "\n",
    "# A data center is considered operational if it does not have the word \"planned\" in the notes\n",
    "datacentermap[\"operational\"] = ~datacentermap[\"notes\"].fillna(\"\").str.contains(\"planned\", case=False)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "datacentermap = datacentermap.drop(columns=[\"total_space\", \"white_space\", \"notes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging\n",
    "\n",
    "Datacentermap has greater accuracy (based on web searches on data center websites), so we remove duplicate data centers from the datacenters_com.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude data centers without the required information\n",
    "datacenters_com = datacenters_com.dropna(subset=[\"total_space_m2\", \"white_space_m2\", \"critical_power_mw\"], how=\"all\")\n",
    "\n",
    "# Exclude data_centers.com entries that appear in the datacentermap data\n",
    "datacenters_com = datacenters_com[~datacenters_com[\"name\"].str.lower().isin(datacentermap[\"name\"].str.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating GeoDataFrames for the data sets\n",
    "datacenters_com_gdf, datacentermap_gdf = (\n",
    "    gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude), crs=WGS84_CRS)\n",
    "    for df in (datacenters_com, datacentermap)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load world map with country borders\n",
    "country_boundaries = gpd.read_file(COUNTRY_BOUNDARIES_INPUT)\n",
    "\n",
    "# Add country iso code column to the datacenter GeoDataFrames\n",
    "datacenters_com_gdf, datacentermap_gdf = (\n",
    "    gpd.sjoin(df, country_boundaries[[\"geometry\", \"ISO_A3\"]], how=\"left\", predicate=\"within\")\n",
    "    for df in (datacenters_com_gdf, datacentermap_gdf)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The country classifications are missing for about 150 data centers, so we export the data, add them manually, and re-import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframes to csv for manual edits\n",
    "datacenters_com_gdf.to_csv(DATACENTERS_COM_GDF_PRE_MANUAL_EDIT_1, index=False)\n",
    "datacentermap_gdf.to_csv(DATACENTERMAP_GDF_PRE_MANUAL_EDIT_1, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport the dataframes after manual edits\n",
    "datacenters_com_gdf = pd.read_csv(DATACENTERS_COM_GDF_POST_MANUAL_EDIT_1)\n",
    "datacentermap_gdf = pd.read_csv(DATACENTERMAP_GDF_POST_MANUAL_EDIT_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During manual inspection, it was observed that due to the manual data collection of datacentermap that some data centers occurred twice, with slight differences in syntax (for instance extra spaces in data center names). Hence, these need to be removed manually, as the data was inconsistent across duplicates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the datacenter names\n",
    "datacentermap_gdf[\"name_stripped\"] = datacentermap_gdf[\"name\"].str.lower().str.replace(r\"\\W+\", \"\", regex=True)\n",
    "\n",
    "# Create a dataframe with the duplicate names for manual inspection\n",
    "duplicate_names_datacentermap = datacentermap_gdf[datacentermap_gdf.duplicated(subset=\"name_stripped\", keep=False)]\n",
    "\n",
    "# Keep the rest of the data without these duplicates\n",
    "datacentermap_without_name_duplicates = datacentermap_gdf.drop_duplicates(subset=\"name_stripped\", keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the duplicate names for manual edits\n",
    "duplicate_names_datacentermap.to_csv(DATACENTERMAP_GDF_DUPLICATES_PRE_MANUAL_EDIT, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manual inspection took place in Excel, and the data was saved as a CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import the corrected duplicates\n",
    "duplicate_names_datacentermap_corrected = pd.read_csv(DATACENTERMAP_GDF_DUPLICATES_POST_MANUAL_EDIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the corrected duplicates with the rest of the data and remove the stripped name column\n",
    "datacentermap_gdf = pd.concat([datacentermap_without_name_duplicates, duplicate_names_datacentermap_corrected]).drop(\n",
    "    columns=[\"name_stripped\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy matching\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates that accrued during multiple geocoding attempts\n",
    "datacenters_com_gdf, datacentermap_gdf = (\n",
    "    df.drop_duplicates(subset=df.columns.difference([\"latitude\", \"longitude\", \"geometry\"]))\n",
    "    for df in (datacenters_com_gdf, datacentermap_gdf)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform fuzzy matching, pre-filtering the data by country to reduce the number of comparisons\n",
    "fuzzy_matches = generate_fuzzy_matches(\n",
    "    datacentermap_gdf, datacenters_com_gdf, match_columns=[\"company\", \"name\"], country_col=\"ISO_A3\", threshold=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance for each matching pair\n",
    "fuzzy_matches[\"distance\"] = fuzzy_matches.apply(\n",
    "    lambda x: haversine(\n",
    "        (x[\"latitude_df1\"], x[\"longitude_df1\"]), (x[\"latitude_df2\"], x[\"longitude_df2\"]), unit=Unit.METERS\n",
    "    ),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "# Remove pairs further than 1150 meters apart. This was found to be a good threshold by manual inspection of the data\n",
    "fuzzy_matches = fuzzy_matches[fuzzy_matches[\"distance\"] < 1150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of the matches for manual inspection\n",
    "subset_columns = [\"company_df1\", \"name_df1\", \"address_df1\", \"company_df2\", \"name_df2\", \"address_df2\", \"best_score\"]\n",
    "fuzzy_matches_manual_edits = fuzzy_matches[subset_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe to csv for manual edits\n",
    "fuzzy_matches_manual_edits.to_csv(FUZZY_MATCHES_PRE_MANUAL_EDIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the fuzzy matches and manually remove those which are not actually matches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport the dataframe after manual edits\n",
    "fuzzy_matches_manual_edits = pd.read_csv(FUZZY_MATCHES_POST_MANUAL_EDIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter fuzzy matches to keep only those from after the manual inspection\n",
    "fuzzy_matches = fuzzy_matches.merge(fuzzy_matches_manual_edits, on=subset_columns, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the fuzzy matching is complete, we update the missing specifications for data centers from datacentermap.com with the specifications from their match in datacenters.com.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating specifications for total space, white space, and critical power\n",
    "fuzzy_matches = fuzzy_matches.fillna(\n",
    "    {\n",
    "        \"total_space_m2_df1\": fuzzy_matches[\"total_space_m2_df2\"],\n",
    "        \"white_space_m2_df1\": fuzzy_matches[\"white_space_m2_df2\"],\n",
    "        \"critical_power_mw_df1\": fuzzy_matches[\"critical_power_mw_df2\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the entries from datacenters.com that also occur in datacentermap.com, based on company, name, and address\n",
    "datacenters_com_key = datacenters_com_gdf[[\"company\", \"name\", \"address\"]].agg(\"-\".join, axis=1)\n",
    "fuzzy_matches_key = fuzzy_matches[[\"company_df2\", \"name_df2\", \"address_df2\"]].agg(\"-\".join, axis=1)\n",
    "\n",
    "datacenters_com_gdf = datacenters_com_gdf[~datacenters_com_key.isin(fuzzy_matches_key)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates in datacentermap.com with the exception of the company column\n",
    "datacentermap_gdf = datacentermap_gdf.drop_duplicates(subset=datacentermap_gdf.columns.difference([\"company\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datacentermap data is then updated based on the data filled in from datacenters.com for the matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge and update values in one go\n",
    "merged_df = datacentermap_gdf.merge(\n",
    "    fuzzy_matches[\n",
    "        [\"company_df1\", \"name_df1\", \"address_df1\", \"total_space_m2_df1\", \"white_space_m2_df1\", \"critical_power_mw_df1\"]\n",
    "    ],\n",
    "    left_on=[\"company\", \"name\", \"address\"],\n",
    "    right_on=[\"company_df1\", \"name_df1\", \"address_df1\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "# Update the three specification columns\n",
    "for original, update in [\n",
    "    (\"total_space_m2\", \"total_space_m2_df1\"),\n",
    "    (\"white_space_m2\", \"white_space_m2_df1\"),\n",
    "    (\"critical_power_mw\", \"critical_power_mw_df1\"),\n",
    "]:\n",
    "    merged_df[original] = merged_df[update].combine_first(merged_df[original])\n",
    "\n",
    "# Keep only original columns\n",
    "datacentermap_gdf = merged_df[datacentermap_gdf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add operational status to all datacenters_com entries\n",
    "datacenters_com_gdf[\"operational\"] = True\n",
    "\n",
    "# Add a new column which identifies which data source the data comes from\n",
    "datacenters_com_gdf[\"data_source\"] = \"datacenters_com\"\n",
    "datacentermap_gdf[\"data_source\"] = \"datacentermap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the databases\n",
    "data_centers = pd.concat([datacenters_com_gdf, datacentermap_gdf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling in missing specs from Amazon, Meta, Google, Apple, and Microsoft\n",
    "\n",
    "These large tech companies do not report data center information such as area or critical power despite being major players in the data center market. Therefore, manual data collection from newspapers and data center blogs on the total data center space was performed. This resulted in data for about 1/3rd of the data centers in question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_5_COMPANIES = [\"Amazon AWS\", \"Meta\", \"Google\", \"Apple\", \"Microsoft\"]\n",
    "AMAZON_AWS = \"Amazon AWS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data centers without total space, nor white space, nor critical power specs\n",
    "data_centers_missing_specs = data_centers[\n",
    "    data_centers[[\"total_space_m2\", \"white_space_m2\", \"critical_power_mw\"]].isna().all(axis=1)\n",
    "]\n",
    "\n",
    "# Separate data centers with specs\n",
    "data_centers_with_specs = data_centers.drop(data_centers_missing_specs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv for manual data additions\n",
    "data_centers_missing_specs.to_csv(DATA_CENTERS_MISSING_SPECS_PRE_MANUAL_EDIT, index=False)\n",
    "data_centers_with_specs.to_csv(DATA_CENTERS_WITH_SPECS_PRE_MANUAL_EDIT, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During further inspection at this stage, it was observed that fuzzy matching was insufficient to catch all matches between the two data sources. Therefore, manual removal of the matches from datacenters.com (while replacing missing data from datacentermap) was performed.\n",
    "\n",
    "For adding information for Apple, Amazon, Meta, and Google which were missing information, internet searches were performed for each of the approximately 300 data centers. Square footage, often from news articles, was collected and added with the source reported in the \"source link\" column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import after manual data additions\n",
    "data_centers_missing_specs = pd.read_csv(DATA_CENTERS_MISSING_SPECS_POST_MANUAL_EDIT)\n",
    "data_centers_with_specs = pd.read_csv(DATA_CENTERS_WITH_SPECS_POST_MANUAL_EDIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data centers with and without specs\n",
    "data_centers = pd.concat([data_centers_with_specs, data_centers_missing_specs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any remaining duplicates based on 'company', 'name, 'address'\n",
    "data_centers = data_centers.drop_duplicates(subset=[\"company\", \"name\", \"address\"])\n",
    "\n",
    "# Convert any remaining strings to floats in the total_space_m2, white_space_m2, critical_power_mw columns\n",
    "columns_to_convert = [\"total_space_m2\", \"white_space_m2\", \"critical_power_mw\"]\n",
    "data_centers[columns_to_convert] = data_centers[columns_to_convert].apply(pd.to_numeric, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final data cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon inspecting the data, some critical power figures from datacenters.com are inconceivable large, even when assumed to be reported in kW and converted to MW. Therefore, for critical power ratings above 300 MW from datacenters.com, we remove this statistic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITICAL_POWER_UPPER_LIMIT = 300\n",
    "DATACENTERS_COM_STR = \"datacenters_com\"\n",
    "\n",
    "# Remove critical power outlier values\n",
    "data_centers.loc[\n",
    "    (data_centers[\"critical_power_mw\"] > CRITICAL_POWER_UPPER_LIMIT)\n",
    "    & (data_centers[\"data_source\"] == DATACENTERS_COM_STR),\n",
    "    \"critical_power_mw\",\n",
    "] = np.nan\n",
    "\n",
    "# Change remaining floats with 0 values to nan\n",
    "for column in [\"total_space_m2\", \"white_space_m2\", \"critical_power_mw\"]:\n",
    "    data_centers.loc[data_centers[column] == 0.0, column] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove data centers with no specs, except when company name is one of the five to be imputed\n",
    "data_centers = data_centers[\n",
    "    data_centers[\"company\"].isin(BIG_5_COMPANIES)\n",
    "    | data_centers[[\"total_space_m2\", \"white_space_m2\", \"critical_power_mw\"]].notna().any(axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to csv\n",
    "data_centers.to_csv(f\"{DATACENTERS_FINAL_OUTPUT_PREFIX}impute_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data imputation\n",
    "\n",
    "For the Big 5 companies, many data centers are still missing information. In order to obtain a more complete picture of data center impacts, a minimum and maximum estimate of floor space is filled in for the locations with missing information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create base dataframes for min, max, and avg estimates\n",
    "total_space_estimates = {scenario: data_centers.copy() for scenario in PowerCapacityScenario}\n",
    "\n",
    "# Companies to process in order\n",
    "companies = BIG_5_COMPANIES\n",
    "\n",
    "# Fill missing total space for each company and aggregation method\n",
    "for scenario, df in total_space_estimates.items():\n",
    "    for company in companies:\n",
    "        result = impute_missing_values(\n",
    "            df=df,\n",
    "            company_name=company,\n",
    "            power_capacity_scenario=scenario,\n",
    "            # Amazon listings can be individual buildings or campuses with multiple data centers.\n",
    "            # We exclude the word \"Campus\" from the name to avoid overestimating the total space.\n",
    "            name_should_not_contain=\"Campus\" if company == AMAZON_AWS else None,\n",
    "            target_column=\"total_space_m2\",\n",
    "        )\n",
    "        total_space_estimates[scenario] = result\n",
    "\n",
    "# Remove NaN values and export\n",
    "for scenario, df in total_space_estimates.items():\n",
    "    df_cleaned = df.dropna(subset=[\"total_space_m2\", \"white_space_m2\", \"critical_power_mw\"], how=\"all\")\n",
    "    df_cleaned.to_csv(f\"{DATACENTERS_FINAL_OUTPUT_PREFIX}impute_{scenario}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
